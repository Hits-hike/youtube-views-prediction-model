{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel\n",
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "embedding_model = BertModel.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_fn(text: List[str]) -> torch.Tensor:\n",
    "    inputs = tokenizer.batch_encode_plus(text)\n",
    "    output = embedding_model(\n",
    "        input_ids = torch.tensor(inputs['input_ids']),\n",
    "        attention_mask = torch.tensor(inputs['attention_mask'])\n",
    "    ).pooler_output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, base_count, embedding_fn, embedding_out):\n",
    "        super().__init__()\n",
    "        self.base_count = base_count\n",
    "        self.embedding_fn = embedding_fn\n",
    "        self.embedding_out = embedding_out\n",
    "        self.embedding_drop = nn.Dropout(.3)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.embedding_out + self.base_count, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.3),\n",
    "#             nn.BatchNorm1d(200),\n",
    "            nn.Linear(200, 70),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.3),\n",
    "#             nn.BatchNorm1d(70),\n",
    "            nn.Linear(70, 1),\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, x_embedding: str):\n",
    "        embedding_out = self.embedding_fn(x_embedding)\n",
    "        embedding_out = self.embedding_drop(embedding_out)\n",
    "        x = torch.cat([embedding_out, x], 1)\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./cap-train.csv')\n",
    "df_test = pd.read_csv('./cap-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = df_train['views'].values\n",
    "train_y[train_y != train_y] = 0\n",
    "train_y[train_y <= 0] = 1\n",
    "train_y_log_max = np.log(train_y.max())\n",
    "train_y = np.log(train_y + np.finfo(train_y.dtype).eps) / train_y_log_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_counts = df_train.subscriber_count.values\n",
    "train_x_embeddings = df_train.preprocessed_title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_counts[train_x_counts != train_x_counts] = 0\n",
    "train_x_counts[train_x_counts <= 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_max = np.log(train_x_counts.max())\n",
    "train_x_counts = np.log(train_x_counts + np.finfo(train_x_counts.dtype).eps) / log_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "def batch_maker(counts, embeddings, ys):\n",
    "    batch_count, batch_embedding, batch_y = [], [], []\n",
    "    for count, embedding, y in zip(counts, embeddings, ys):\n",
    "        batch_count.append([count])\n",
    "        batch_embedding.append(embedding)\n",
    "        batch_y.append(y)\n",
    "        \n",
    "        if len(batch_count) == batch_size:\n",
    "            yield batch_count, batch_embedding, batch_y\n",
    "            batch_count, batch_embedding, batch_y = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .001\n",
    "decay = .0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(1, embedding_fn, 768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1, train_x2, ys = train_x_counts, train_x_embeddings, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optim, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        total, total_loss = 0, 0\n",
    "        for x1, x2 , y in batch_maker(train_x1, train_x2, ys):\n",
    "            x1 = torch.from_numpy(np.array(x1, dtype=np.float32))\n",
    "            y = torch.from_numpy(np.array(y, dtype=np.float32))\n",
    "            x1 = x1.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            pred = model(x1, x2)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            total += 1\n",
    "            total_loss += loss.item()\n",
    "            print(f'loss: {loss}')\n",
    "        print(f'trainig loss: {total_loss / total}')\n",
    "#         if not (ep % 10):\n",
    "#             validation(model, criterion, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.021554136648774147\n",
      "loss: 0.021419446915388107\n",
      "loss: 0.035821333527565\n",
      "loss: 0.00015687169798184186\n",
      "loss: 0.0036772647872567177\n",
      "loss: 0.0105270491912961\n",
      "loss: 0.10175984352827072\n",
      "loss: 0.02936566434800625\n",
      "loss: 0.010524761863052845\n",
      "loss: 0.02857932634651661\n",
      "loss: 0.011014967225492\n",
      "loss: 3.744450805243105e-05\n",
      "loss: 0.01181840617209673\n",
      "loss: 0.022864053025841713\n",
      "loss: 0.0002868060255423188\n",
      "loss: 0.0060671367682516575\n",
      "loss: 0.019967464730143547\n",
      "loss: 6.70112858642824e-05\n",
      "loss: 0.025042658671736717\n",
      "loss: 0.004553244449198246\n",
      "loss: 1.092250386136584e-06\n",
      "loss: 0.021938100457191467\n",
      "loss: 0.0005671984981745481\n",
      "loss: 0.01293319370597601\n",
      "loss: 0.02573792263865471\n",
      "loss: 0.0351872593164444\n",
      "loss: 0.0013290608767420053\n",
      "loss: 0.023734411224722862\n",
      "loss: 0.0011931638000532985\n",
      "loss: 0.006235924549400806\n",
      "loss: 0.02742835506796837\n",
      "loss: 0.004610045813024044\n",
      "loss: 0.004695686046034098\n",
      "loss: 0.0044922553934156895\n",
      "loss: 0.01698998734354973\n",
      "loss: 0.01159173995256424\n",
      "loss: 0.00247577135451138\n",
      "loss: 0.017568886280059814\n",
      "loss: 0.2586989998817444\n",
      "loss: 0.0006939090671949089\n",
      "loss: 0.003262765472754836\n",
      "loss: 0.0007368493243120611\n",
      "loss: 0.022608233615756035\n",
      "loss: 0.0001320759765803814\n",
      "loss: 0.00011046192958019674\n",
      "loss: 0.014956378377974033\n",
      "loss: 0.01712486334145069\n",
      "loss: 0.021771393716335297\n",
      "loss: 0.02817831002175808\n",
      "loss: 0.028335154056549072\n",
      "loss: 0.024266235530376434\n",
      "loss: 0.014199644327163696\n",
      "loss: 0.027168957516551018\n",
      "loss: 0.003564240876585245\n",
      "loss: 0.014547239057719707\n",
      "loss: 0.0036795565392822027\n",
      "loss: 0.00043982284842059016\n",
      "loss: 0.00348883424885571\n",
      "loss: 0.008301269263029099\n",
      "loss: 2.7421249342296505e-06\n",
      "loss: 0.04379082843661308\n",
      "loss: 0.0025377951096743345\n",
      "loss: 0.004945750348269939\n",
      "loss: 0.012808723375201225\n",
      "loss: 0.015876177698373795\n",
      "loss: 0.001216891803778708\n",
      "loss: 0.016765087842941284\n",
      "loss: 0.007524864748120308\n",
      "loss: 0.049153100699186325\n",
      "loss: 0.005073338281363249\n",
      "loss: 0.0045048245228827\n",
      "loss: 0.003026817226782441\n",
      "loss: 0.006008724216371775\n",
      "loss: 0.023975368589162827\n",
      "loss: 0.006063701584935188\n",
      "loss: 0.009777241386473179\n",
      "loss: 0.04329795390367508\n",
      "loss: 0.00026867593987844884\n",
      "loss: 0.016160286962985992\n",
      "loss: 0.004882899112999439\n",
      "loss: 4.066931978741195e-06\n",
      "loss: 0.025789031758904457\n",
      "loss: 0.006421883590519428\n",
      "loss: 0.003577996976673603\n",
      "loss: 0.006151194218546152\n",
      "loss: 0.0008612727979198098\n",
      "loss: 0.003266038140282035\n",
      "loss: 0.0003994010330643505\n",
      "loss: 0.010772558860480785\n",
      "loss: 0.005133551545441151\n",
      "loss: 0.0005668181693181396\n",
      "loss: 0.003418874694034457\n",
      "loss: 0.0037242791149765253\n",
      "loss: 0.0010012778220698237\n",
      "loss: 0.023239247500896454\n",
      "loss: 0.0004282448207959533\n",
      "loss: 0.008514685556292534\n",
      "loss: 0.0008093218202702701\n",
      "loss: 0.00483926385641098\n",
      "loss: 0.0034688026644289494\n",
      "loss: 0.0016012202249839902\n",
      "loss: 0.002757537178695202\n",
      "loss: 0.0011107136961072683\n",
      "loss: 0.0015603958163410425\n",
      "loss: 0.006761778146028519\n",
      "loss: 0.0023986694868654013\n",
      "loss: 0.005430554039776325\n",
      "loss: 0.017607999965548515\n",
      "loss: 0.01031704805791378\n",
      "loss: 0.005557440686970949\n",
      "loss: 0.011015880852937698\n",
      "loss: 0.0012037284905090928\n",
      "loss: 0.0015823172871023417\n",
      "loss: 0.006037561688572168\n",
      "loss: 5.045382636126305e-07\n",
      "loss: 0.0049029868096113205\n",
      "loss: 0.0007984452531673014\n",
      "loss: 0.022545453161001205\n",
      "loss: 0.03644150123000145\n",
      "loss: 0.007184593006968498\n",
      "loss: 0.002487149089574814\n",
      "loss: 0.0003209540154784918\n",
      "loss: 0.014339382760226727\n",
      "loss: 0.014377529732882977\n",
      "loss: 0.0013274403754621744\n",
      "loss: 0.0023012578021734953\n",
      "loss: 9.686923294793814e-05\n",
      "loss: 0.0009501762688159943\n",
      "loss: 3.1047416996443644e-05\n",
      "loss: 0.00019938990590162575\n",
      "loss: 0.002889867639169097\n",
      "loss: 0.005783314350992441\n",
      "loss: 0.0007388942176476121\n",
      "loss: 0.005144413094967604\n",
      "loss: 0.027857353910803795\n",
      "loss: 0.00034351483918726444\n",
      "loss: 0.0164487287402153\n",
      "loss: 0.0033276781905442476\n",
      "loss: 0.01546377781778574\n",
      "loss: 0.008338021114468575\n",
      "loss: 6.910919836400353e-08\n",
      "loss: 0.0022345148026943207\n",
      "loss: 0.0016123492969200015\n",
      "loss: 0.0004040398634970188\n",
      "loss: 0.0008088538888841867\n",
      "loss: 0.0017485900316387415\n",
      "loss: 0.00023243881878443062\n",
      "loss: 6.748100713593885e-05\n",
      "loss: 2.9066755814710632e-05\n",
      "loss: 0.02251298539340496\n",
      "loss: 0.02299479767680168\n",
      "loss: 0.0012471518712118268\n",
      "loss: 0.00039496191311627626\n",
      "loss: 0.005773745011538267\n",
      "loss: 0.005313675384968519\n",
      "loss: 0.010273715481162071\n",
      "loss: 0.002304301131516695\n",
      "loss: 5.8892695960821584e-05\n",
      "loss: 8.188976789824665e-05\n",
      "loss: 0.010402552783489227\n",
      "loss: 0.003505781991407275\n",
      "loss: 4.403008279041387e-05\n",
      "loss: 0.01850108429789543\n",
      "loss: 0.01685524173080921\n",
      "loss: 0.0040090009570121765\n",
      "loss: 0.0035036965273320675\n",
      "loss: 0.009729702025651932\n",
      "loss: 5.444957423605956e-05\n",
      "loss: 0.08244626969099045\n",
      "loss: 0.0015483829192817211\n",
      "loss: 0.0008211564272642136\n",
      "loss: 0.025910401716828346\n",
      "loss: 0.01524871401488781\n",
      "loss: 0.01238116156309843\n",
      "loss: 0.0031951051205396652\n",
      "loss: 4.503831974034256e-08\n",
      "loss: 0.0001446335663786158\n",
      "loss: 0.0031587781850248575\n",
      "loss: 0.02145736664533615\n",
      "loss: 0.024805178865790367\n",
      "loss: 0.00032356049632653594\n",
      "loss: 3.752552220248617e-05\n",
      "loss: 0.006091260816901922\n",
      "loss: 0.005449734628200531\n",
      "loss: 0.003443594090640545\n",
      "loss: 0.00017294853751081973\n",
      "loss: 0.021233486011624336\n",
      "loss: 0.0007764561451040208\n",
      "loss: 0.001991415861994028\n",
      "loss: 0.010275304317474365\n",
      "loss: 0.0030983486212790012\n",
      "loss: 0.037729062139987946\n",
      "loss: 0.010585656389594078\n",
      "loss: 0.0013645619619637728\n",
      "loss: 0.005458538886159658\n",
      "loss: 4.07174138672417e-06\n",
      "loss: 0.0018962195608764887\n",
      "loss: 0.0006693692412227392\n",
      "loss: 0.0001698630367172882\n",
      "loss: 0.00636807968840003\n",
      "loss: 0.004271019250154495\n",
      "loss: 0.006832654122263193\n",
      "loss: 0.0022685611620545387\n",
      "loss: 1.3289956768858247e-05\n",
      "loss: 0.001195137039758265\n",
      "loss: 9.78042407950852e-07\n",
      "loss: 0.00837833434343338\n",
      "loss: 0.032552532851696014\n",
      "loss: 3.393112274352461e-05\n",
      "loss: 0.004330237861722708\n",
      "loss: 0.005730379372835159\n",
      "loss: 0.0041987961158156395\n",
      "loss: 0.025298206135630608\n",
      "loss: 0.0022226462606340647\n",
      "loss: 0.001585007063113153\n",
      "loss: 0.0172707661986351\n",
      "loss: 0.007355052977800369\n",
      "loss: 0.0012860380811616778\n",
      "loss: 0.01049982849508524\n",
      "loss: 0.00016822323959786445\n",
      "loss: 0.0012633998412638903\n",
      "loss: 0.0008249834063462913\n",
      "loss: 0.01129394955933094\n",
      "loss: 0.00032695737900212407\n",
      "loss: 0.00012355945364106447\n",
      "loss: 0.0007192039047367871\n",
      "loss: 0.0058189015835523605\n",
      "loss: 0.025056978687644005\n",
      "loss: 0.00037329047336243093\n",
      "loss: 0.03539326414465904\n",
      "loss: 3.265455598011613e-05\n",
      "loss: 0.015553035773336887\n",
      "loss: 0.006802067626267672\n",
      "loss: 1.1803018423961475e-05\n",
      "loss: 0.0025385459885001183\n",
      "loss: 0.0126656424254179\n",
      "loss: 0.009523388929665089\n",
      "loss: 0.03336675837635994\n",
      "loss: 2.6264260668540373e-05\n",
      "loss: 0.008141805417835712\n",
      "loss: 0.0031571167055517435\n",
      "loss: 0.012715007178485394\n",
      "loss: 0.010611833073198795\n",
      "loss: 0.023887403309345245\n",
      "loss: 0.005379891954362392\n",
      "loss: 0.06896433234214783\n",
      "loss: 0.011369416490197182\n",
      "loss: 0.00028639432275667787\n",
      "loss: 0.050827156752347946\n",
      "loss: 0.02785426937043667\n",
      "loss: 0.014029876329004765\n",
      "loss: 0.00023331839474849403\n",
      "loss: 0.03592964634299278\n",
      "loss: 0.001351966173388064\n",
      "loss: 0.0012926728231832385\n",
      "loss: 0.04278726503252983\n",
      "loss: 0.0016807123320177197\n",
      "loss: 0.0010442216880619526\n",
      "loss: 0.04327500984072685\n",
      "loss: 0.010848786681890488\n",
      "loss: 0.0016776153352111578\n",
      "loss: 0.0035010685678571463\n",
      "loss: 0.014143731445074081\n",
      "loss: 0.0161878801882267\n",
      "loss: 0.012549659237265587\n",
      "loss: 0.02057635225355625\n",
      "loss: 0.12180735170841217\n",
      "loss: 0.014343522489070892\n",
      "loss: 0.00010047243267763406\n",
      "loss: 0.0006831237697042525\n",
      "loss: 0.006198701914399862\n",
      "loss: 0.00038930002483539283\n",
      "loss: 0.011713163927197456\n",
      "loss: 0.006232677027583122\n",
      "loss: 0.06413891166448593\n",
      "loss: 0.0038064103573560715\n",
      "loss: 0.02725199982523918\n",
      "loss: 0.008252108469605446\n",
      "loss: 0.012406840920448303\n",
      "loss: 0.013641810975968838\n",
      "loss: 0.05338036268949509\n",
      "loss: 1.1522929526108783e-05\n",
      "loss: 0.016178537160158157\n",
      "loss: 0.0523454025387764\n",
      "loss: 0.03846091404557228\n",
      "loss: 0.001562718185596168\n",
      "loss: 0.007690392434597015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-446-0d72cffd087e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-445-f3d8c90cf30a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, optim, criterion)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, epochs, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
